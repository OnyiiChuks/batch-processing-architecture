#version: "3.8"
services:

  #Service for hdfs namenode
  hdfs-namenode:
    image: bde2020/hadoop-namenode:2.0.0-hadoop3.2.1-java8
    container_name: hdfs-namenode
    ports:
      - "9870:9870"  # HDFS Web UI
      - "9000:9000"  # NameNode RPC
    environment:
      - CLUSTER_NAME=hadoop-cluster
      - CORE_CONF_fs_defaultFS=hdfs://hdfs-namenode:9000
      - CORE_CONF_hadoop_http_staticuser_user=root
      - HDFS_CONF_dfs_replication=1
    volumes:
      - namenode-data:/hadoop/dfs/name
      - ./hdfs_docker/data:/mnt/data #for data exchange with spark
      - ./hdfs_docker/namenode/core-site.xml:/opt/hadoop/etc/hadoop/core-site.xml
      - ./hdfs_docker/namenode/hdfs-site.xml:/opt/hadoop/etc/hadoop/hdfs-site.xml
    networks:
      - spark-hdfs-network
    healthcheck:
      test: curl -f http://localhost:9870 || exit 1
      interval: 30s
      timeout: 10s
      retries: 3


#service for hdfs datanode
  hdfs-datanode:
    image: bde2020/hadoop-datanode:2.0.0-hadoop3.2.1-java8
    container_name: hdfs-datanode
    environment:
      - CLUSTER_NAME=hadoop-cluster
      - CORE_CONF_fs_defaultFS=hdfs://hdfs-namenode:9000
      - CORE_CONF_hadoop_http_staticuser_user=root
      - HDFS_CONF_dfs_replication=1
    volumes:
      - datanode-data:/hadoop/dfs/data
      - ./hdfs_docker/datanode/core-site.xml:/opt/hadoop/etc/hadoop/core-site.xml
      - ./hdfs_docker/datanode/hdfs-site.xml:/opt/hadoop/etc/hadoop/hdfs-site.xml
    networks:
      - spark-hdfs-network
    healthcheck:
      test: curl -f http://localhost:9864 || exit 1
      interval: 30s
      timeout: 10s
      retries: 3


#service for Spark Master
  spark-master:
    image: bde2020/spark-master:3.1.1-hadoop3.2
    container_name: spark-master
    environment:
      - SPARK_MODE=master
      - SPARK_MASTER_HOST=spark-master
      - SPARK_HOME=/spark
    ports:
      - "8080:8080"  # Spark Master UI
      - "7077:7077"  # Spark Master Port
    networks:
      - spark-hdfs-network
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080/"]
      interval: 30s
      timeout: 10s
      retries: 3


#service for Spark Worker
  spark-worker:
    image: bde2020/spark-worker:3.1.1-hadoop3.2
    container_name: spark-worker
    environment:
      - SPARK_MODE=worker
      - SPARK_MASTER_URL=spark://spark-master:7077
      - SPARK_HOME=/spark
    depends_on:
    - spark-master  
    networks:
      - spark-hdfs-network



#service for spark ingestion client
  spark-ingestion:
    build:
     context: ./spark_ingestion  # Dockerfile in the spark_ingestion directory
     #args:      # Pass environment variables to Dockerfile
        #ENCRYPTION_KEY: ${SECRET-kEY}
    container_name: spark-ingestion
    profiles: ["manual"]   #ensures that container is started by the orchestrator and not dockercompose
    tty: true
    env_file:
      - ./spark_ingestion/.env
    environment:
      - SPARK_MODE=client
      - SPARK_MASTER_URL=spark://spark-master:7077
      - HDFS_NAMENODE=hdfs://hdfs-namenode:9000  
      - PYTHONUNBUFFERED=1  # Ensure logs are streamed in real-time
      - BATCH_NUMBER=${BATCH_NUMBER}
    depends_on:
      spark-master:
        condition: service_healthy
      hdfs-namenode:
        condition: service_healthy
    volumes:
      - ./spark_ingestion:/app/ingestion  # Mount spark_ingestion folder
      - ./data/raw_fraud.csv:/app/ingestion/data/raw_fraud.csv    # Load the CSV file
      - ./spark_ingestion/logs/:/app/ingestion/logs
    networks:
      - spark-hdfs-network 
    #The script runs along with the log4j.properties which contains settings on where logs are to be displayed
    command: ["/opt/spark/bin/spark-submit", "--master", "spark://spark-master:7077",
        "--conf", "spark.driver.extraJavaOptions=-Dlog4j.configuration=file:/app/ingestion/log4j.properties",
        "--conf", "spark.executor.extraJavaOptions=-Dlog4j.configuration=file:/app/ingestion/log4j.properties", 
        "/app/ingestion/ingestion.py",
        "${BATCH_NUMBER}"]
    
      

    
#service for PostgreSQL Database
  postgres:
    build:
      context: ./postgres_db
      args:      # Pass environment variables to Dockerfile
        POSTGRES_USER: ${POSTGRES_USER}
        POSTGRES_PASSWORD: ${POSTGRES_PASSWORD}
        POSTGRES_DB: ${POSTGRES_DB}
    container_name: postgres-db
    env_file:
      - ./postgres_db/db_var.env
    environment:
      - POSTGRES_LOGGING=stderr
    ports:
      - "5432:5432"
    volumes:
      - postgres-data:/var/lib/postgresql/data
      - ./postgres_db/init-db.sql:/docker-entrypoint-initdb.d/init-db.sql  # Databases persistence
      - ./postgres_db/logs:/var/log/postgresql  # postgreSql logs will be saved here
      - ./postgres_db/conf/postgresql.conf:/etc/postgresql/postgresql.conf
      #ssl certificate folders
      - ./postgres_db/ssl/server.crt:/var/lib/postgresql/ssl/server.crt
      - ./postgres_db/ssl/server.key:/var/lib/postgresql/ssl/server.key
      - ./postgres_db/conf/pg_hba.conf:/var/lib/postgresql/conf/pg_hba.conf 
    healthcheck:
      test: ["CMD-SHELL", "sh -c 'pg_isready -U ${POSTGRES_USER} -d ${POSTGRES_DB}'"]
      interval: 30s
      timeout: 10s
      retries: 5
    command: postgres -c config_file=/etc/postgresql/postgresql.conf #loading the .conf file for log settings
    
    networks:
      - spark-hdfs-network
    


#service for Spark Transformation Client
  spark-transformation:
    build:
      context: ./spark_transformation  
      args:      # Pass environment variables to Dockerfile
        DB_USER: ${DB_USER}
        DB_PASSWORD: ${DB_PASSWORD}
    container_name: spark-transformation
    profiles: ["manual"]    #ensure that container is started by the orchestrator and not dockercompose
    env_file:
      - ./spark_transformation/.env
    environment:
      - SPARK_MODE=client
      - SPARK_MASTER_URL=spark://spark-master:7077
      - JAVA_HOME=/usr/lib/jvm/java-11-openjdk-amd64  
      - HDFS_NAMENODE=hdfs://hdfs-namenode:9000
      - BATCH_NUMBER=${BATCH_NUMBER}
    volumes:
      - ./spark_transformation:/opt/spark/scripts
      - ./spark_transformation/logs/:/opt/spark/scripts/logs
    depends_on:
       spark-master:
          condition: service_healthy
       hdfs-namenode:
          condition: service_healthy
       postgres:
          condition: service_healthy
    networks:
      - spark-hdfs-network
    command: [ "spark-submit",
         "--conf", "spark.driver.extraJavaOptions=-Dlog4j.configuration=file:/opt/spark/scripts/log4j.properties",  #Ensures the Spark driver uses the specified log4j properties file.
            "--conf", "spark.executor.extraJavaOptions=-Dlog4j.configuration=file:/opt/spark/scripts/log4j.properties", #Ensures the Spark executors also use the log4j properties file.
             "/opt/spark/scripts/transformation.py", "${BATCH_NUMBER}"]  #path to transformation script to be executed and the batch number.
    
  

#service for Data Visualization
  visualization:
    build:
      context: ./data_visualization   #Service folder
    container_name: visualization
    env_file:
      - ./data_visualization/.env
    ports:
      - "8050:8050"  # Dash default port
    depends_on:
      - postgres    # Ensure the PostgreSQL service is ready
    networks:
      - spark-hdfs-network 
    volumes:
      - ./data_visualization:/app  # Mount the app directory  
    profiles: ["manual"]   #ensure that container is started by the orchestrator and not dockercompose
    command: >
      sh -c "pip install dash pandas sqlalchemy && python /app/dash_app.py"  
     

#Volumns for persisting data
volumes:
  namenode-data:
  datanode-data:
  postgres-data:
  

#the name of network in use
networks:
  spark-hdfs-network:
    driver: bridge
